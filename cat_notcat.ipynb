{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cat_notcat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yWKHbTQHwdz",
        "outputId": "47593e47-f619-48c7-c1dd-cfc451b91cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', '.ipynb_checkpoints', 'val']\n",
            "other\n",
            "cat\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "\n",
        "# Read input images and assign labels based on folder names\n",
        "print(os.listdir(\"/content/data\"))\n",
        "\n",
        "SIZE = 128  #Resize images\n",
        "\n",
        "#Capture training data and labels into respective lists\n",
        "train_images = []\n",
        "train_labels = [] \n",
        "\n",
        "for directory_path in glob.glob(\"/content/data/train/*\"):\n",
        "    label = directory_path.split(\"/\")[-1]\n",
        "    print(label)\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        #print(img_path)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        train_images.append(img)\n",
        "        train_labels.append(label)\n",
        "\n",
        "#Convert lists to arrays        \n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "# Capture test/validation data and labels into respective lists\n",
        "\n",
        "test_images = []\n",
        "test_labels = [] \n",
        "for directory_path in glob.glob(\"/content/data/val/*\"):\n",
        "    fruit_label = directory_path.split(\"/\")[-1]\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        test_images.append(img)\n",
        "        test_labels.append(fruit_label)\n",
        "\n",
        "#Convert lists to arrays                \n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "#Encode labels from text to integers.\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(test_labels)\n",
        "test_labels_encoded = le.transform(test_labels)\n",
        "le.fit(train_labels)\n",
        "train_labels_encoded = le.transform(train_labels)\n",
        "\n",
        "#Split data into test and train datasets (already split but assigning to meaningful convention)\n",
        "x_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded\n",
        "\n",
        "###################################################################\n",
        "# Normalize pixel values to between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "#One hot encode y values for neural network. \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "#############################\n",
        "#Load model wothout classifier/fully connected layers\n",
        "VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
        "\n",
        "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
        "for layer in VGG_model.layers:\n",
        "\tlayer.trainable = False\n",
        "    \n",
        "VGG_model.summary()  #Trainable parameters will be 0\n",
        "\n",
        "\n",
        "#Now, let us use features from convolutional network for RF\n",
        "feature_extractor=VGG_model.predict(x_train)\n",
        "\n",
        "features = feature_extractor.reshape(feature_extractor.shape[0], -1)\n",
        "\n",
        "X_for_RF = features #This is our X input to RF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "RF_model = SVC(kernel='linear')\n",
        "\n",
        "# Train the model on training data\n",
        "RF_model.fit(X_for_RF, y_train) #For sklearn no one hot encoding\n",
        "\n",
        "#Send test data through same feature extractor process\n",
        "X_test_feature = VGG_model.predict(x_test)\n",
        "X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)\n",
        "\n",
        "#Now predict using the trained RF model. \n",
        "prediction_RF = RF_model.predict(X_test_features)\n",
        "#Inverse le transform to get original label back. \n",
        "prediction_RF = le.inverse_transform(prediction_RF)\n",
        "\n",
        "#Print overall accuracy\n",
        "from sklearn import metrics\n",
        "print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction_RF))\n",
        "\n",
        "#Confusion Matrix - verify accuracy of each class\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(test_labels, prediction_RF)\n",
        "#print(cm)\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "#Check results on a few select images\n",
        "n=np.random.randint(0, x_test.shape[0])\n",
        "img = x_test[n]\n",
        "plt.imshow(img)\n",
        "input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
        "input_img_feature=VGG_model.predict(input_img)\n",
        "input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)\n",
        "prediction_RF = RF_model.predict(input_img_features)[0] \n",
        "prediction_RF = le.inverse_transform([prediction_RF])  #Reverse the label encoder to original name\n",
        "print(\"The prediction for this image is: \", prediction_RF)\n",
        "print(\"The actual label for this image is: \", test_labels[n])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "kNmMROrQL636",
        "outputId": "18b1acf6-e073-4e1d-cf24-62f7102aca91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy =  1.0\n",
            "The prediction for this image is:  ['cat']\n",
            "The actual label for this image is:  cat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9qfTw4gMYNp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
